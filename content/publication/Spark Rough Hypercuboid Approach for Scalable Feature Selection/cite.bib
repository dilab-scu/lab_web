@article{9537667,
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  volume={},
  pages={1-1},
  year={2021},
  issn={1041-4347},
  doi={10.1109/TKDE.2021.3112520},
  url={https://ieeexplore.ieee.org/abstract/document/9537667},
  title={Spark Rough Hypercuboid Approach for Scalable Feature Selection}, 
  author={Luo, Chuan and Wang, Sizhao and Li, Tianrui and Chen, Hongmei and Lv, Jian Cheng and Zhang, Yi},
  keywords={Rough hypercuboid, feature selection, parallel computing, scalability, Apache Spark},
  abstract={Feature selection refers to choose an optimal non-redundant feature subset with minimal degradation of learning performance and maximal avoidance of data overfitting. The appearance of large data explosion leads to the sequential execution of algorithms are extremely time-consuming, which necessitates the scalable parallelization of algorithms by efficiently exploiting the distributed computational capabilities. In this paper, we present parallel feature selection algorithms underpinned by a rough hypercuboid approach in order to scale for the growing data volumes. Metrics in terms of rough hypercuboid are highly suitable to parallel distributed processing, and fits well with the Apache Spark cluster computing paradigm. Two data parallelism strategies, namely, vertical partitioning and horizontal partitioning, are implemented respectively to decompose the data into concurrent iterative computing streams. Experimental results on representative datasets show that our algorithms significantly faster than its original sequential counterpart while guaranteeing the quality of the results. Furthermore, the proposed algorithms are perfectly capable of exploiting the distributed-memory clusters to accomplish the computation task that fails on a single node due to the memory constraints. Parallel scalability and extensibility analysis have confirmed that our parallelization extends well to process massive amount of data and can scales well with the increase of computational nodes.}
  }
