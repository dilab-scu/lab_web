<<<<<<< HEAD
@ARTICLE{9305984,
  author={Q. {Ye} and Y. {Sun} and J. {Zhang} and J. C. {Lv}},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={A Distributed Framework For EA-Based NAS}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  abstract={Evolutionary Algorithms(EA) are widely applied in Neural Architecture Search(NAS) and have achieved appealing results. Different EA-based NAS algorithms may utilize different encoding schemes for network representation, while they have the same workflow (i.e., the initialization of the population, individual evaluation, and evolution). Because each individual needs complete training and validation on the target dataset, the EA-based NAS always consumes significant computation and time inevitably, which results in the bottleneck of this approach. To ameliorate this issue, this paper proposes a distributed framework to boost the computing of the EA-based NAS. This framework is a server/worker model where the server distributes individuals, collects the validated individuals and hosts the evolution operations. Meanwhile, the most time-consuming phase (i.e., individual evaluation) is allocated to the computational workers. Additionally, a new packet structure of the message delivered in the cluster is designed to encapsulate various network representation of different EA-based NAS algorithms. We design an EA-based NAS algorithm as a sample to investigate the effectiveness of the proposed framework. Extensive experiments are performed on an illustrative cluster with different scales, and the results reveal that the framework can achieve a nearly linear reduction of the training time with the increase of the computational workers.},
  keywords={Computer architecture;Training;Statistics;Sociology;Neural networks;Clustering algorithms;Servers;distributed framework;evolutionary algorithm (EA);neural architecture search (NAS);evolutionary neural networks},
  doi={10.1109/TPDS.2020.3046774},
  ISSN={1558-2183},
  month={},}
=======
<<<<<<< HEAD
@ARTICLE{9305984,
  author={Q. {Ye} and Y. {Sun} and J. {Zhang} and J. C. {Lv}},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={A Distributed Framework For EA-Based NAS}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  abstract={Evolutionary Algorithms(EA) are widely applied in Neural Architecture Search(NAS) and have achieved appealing results. Different EA-based NAS algorithms may utilize different encoding schemes for network representation, while they have the same workflow (i.e., the initialization of the population, individual evaluation, and evolution). Because each individual needs complete training and validation on the target dataset, the EA-based NAS always consumes significant computation and time inevitably, which results in the bottleneck of this approach. To ameliorate this issue, this paper proposes a distributed framework to boost the computing of the EA-based NAS. This framework is a server/worker model where the server distributes individuals, collects the validated individuals and hosts the evolution operations. Meanwhile, the most time-consuming phase (i.e., individual evaluation) is allocated to the computational workers. Additionally, a new packet structure of the message delivered in the cluster is designed to encapsulate various network representation of different EA-based NAS algorithms. We design an EA-based NAS algorithm as a sample to investigate the effectiveness of the proposed framework. Extensive experiments are performed on an illustrative cluster with different scales, and the results reveal that the framework can achieve a nearly linear reduction of the training time with the increase of the computational workers.},
  keywords={Computer architecture;Training;Statistics;Sociology;Neural networks;Clustering algorithms;Servers;distributed framework;evolutionary algorithm (EA);neural architecture search (NAS);evolutionary neural networks},
  doi={10.1109/TPDS.2020.3046774},
  ISSN={1558-2183},
  month={},}
=======
@ARTICLE{9305984,
  author={Q. {Ye} and Y. {Sun} and J. {Zhang} and J. C. {Lv}},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={A Distributed Framework For EA-Based NAS}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  abstract={Evolutionary Algorithms(EA) are widely applied in Neural Architecture Search(NAS) and have achieved appealing results. Different EA-based NAS algorithms may utilize different encoding schemes for network representation, while they have the same workflow (i.e., the initialization of the population, individual evaluation, and evolution). Because each individual needs complete training and validation on the target dataset, the EA-based NAS always consumes significant computation and time inevitably, which results in the bottleneck of this approach. To ameliorate this issue, this paper proposes a distributed framework to boost the computing of the EA-based NAS. This framework is a server/worker model where the server distributes individuals, collects the validated individuals and hosts the evolution operations. Meanwhile, the most time-consuming phase (i.e., individual evaluation) is allocated to the computational workers. Additionally, a new packet structure of the message delivered in the cluster is designed to encapsulate various network representation of different EA-based NAS algorithms. We design an EA-based NAS algorithm as a sample to investigate the effectiveness of the proposed framework. Extensive experiments are performed on an illustrative cluster with different scales, and the results reveal that the framework can achieve a nearly linear reduction of the training time with the increase of the computational workers.},
  keywords={Computer architecture;Training;Statistics;Sociology;Neural networks;Clustering algorithms;Servers;distributed framework;evolutionary algorithm (EA);neural architecture search (NAS);evolutionary neural networks},
  doi={10.1109/TPDS.2020.3046774},
  ISSN={1558-2183},
  month={},}
>>>>>>> d1744224f7b63c92bf082641033beca38038df52
>>>>>>> ac81028 (update2022)
