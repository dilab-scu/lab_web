<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 5.0.0-beta.0 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jiancheng Lv">

  
  
  
    
  
  <meta name="description" content="Dean and professor of Computer Science of Sichuan University">

  
  <link rel="alternate" hreflang="en-us" href="https://www.dicalab.cn/publication/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css">

  




  

  


  
  

  
  <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="DICAlab">
  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu9b11f55f68445448a3fe0494b605b2d6_13188_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu9b11f55f68445448a3fe0494b605b2d6_13188_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://www.dicalab.cn/publication/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="DICAlab">
  <meta property="og:url" content="https://www.dicalab.cn/publication/">
  <meta property="og:title" content="Publications | DICAlab">
  <meta property="og:description" content="Dean and professor of Computer Science of Sichuan University"><meta property="og:image" content="https://www.dicalab.cn/images/icon_hu9b11f55f68445448a3fe0494b605b2d6_13188_512x512_fill_lanczos_center_3.png">
  <meta property="twitter:image" content="https://www.dicalab.cn/images/icon_hu9b11f55f68445448a3fe0494b605b2d6_13188_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us">
  
    <meta property="og:updated_time" content="2022-04-07T00:00:00&#43;00:00">
  

  




  


  





  <title>Publications | DICAlab</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  ">

  
  
  
  
  
  <script src="/js/wowchemy-init.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">DICAlab</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">DICAlab</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#introduction"><span>Introduction</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#phd_thesis"><span>Ph.D.Thesis</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#md_thesis"><span>M.D.Thesis</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#datasets"><span>Datasets</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#team"><span>Team</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-2">
              Journal article
            </option>
            
            <option value=".pubtype-3">
              Preprint
            </option>
            
            <option value=".pubtype-7">
              Thesis
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2021">
              2021
            </option>
            
            <option value=".year-2020">
              2020
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            <option value=".year-2017">
              2017
            </option>
            
            <option value=".year-2015">
              2015
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/qingye/">QingYe</a></span>, <span ><a href="/author/yuxin-tian/">Yuxin Tian</a></span>, <span ><a href="/author/junweihan/">JunweiHan</a></span>, <span ><a href="/author/fengwu/">FengWu</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/brain-inspired-large-scale-deep-neural-network-system/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/brain-inspired-large-scale-deep-neural-network-system/">
    <div class="img-hover-zoom"><img src="/publication/brain-inspired-large-scale-deep-neural-network-system/featured_hue7f43840b72d2e9b6acb731048752130_206604_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Brain-Inspired Large-Scale Deep Neural Network System."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/brain-inspired-large-scale-deep-neural-network-system/">Brain-Inspired Large-Scale Deep Neural Network System.</a>
  </h3>

  
  <a href="/publication/brain-inspired-large-scale-deep-neural-network-system/" class="summary-link">
    <div class="article-style">
      <p>Large-scale deep neural networks (DNNs) exhibit powerful end-to-end representation and infinite approximation of nonlinear functions, showing excellent performance in several fields and becoming an important development direction. For example, the natural language processing model GPT, after years of development, now has 175 billion network parameters and achieves state-of-the-art performance on several NLP benchmarks. However, according to the existing deep neural network organization, the current large-scale network is difficult to reach the scale of human brain biological neural network connection. At the same time, the existing large-scale DNNs do not perform well in multi-channel collaborative processing, knowledge storage, and reasoning. In this paper, we propose a brain-inspired large-scale DNN model, which is inspired by the division and the functional mechanism of brain regions and built modularly by the functional of the brain, integrates a large amount of existing data and pre-trained models, and proposes the corresponding learning algorithm by the functional mechanism of the brain. The DNN model implements a pathway to automatically build a DNN as an output using the scene as an input. Simultaneously, it should not only learn the correlation between input and output but also needs to have the multi-channel collaborative processing capability to improve the correlation quality, thereby realizing knowledge storage and reasoning ability, which could be treated as a way toward general artificial intelligence. The whole model and all data sets and brain-inspired functional areas are managed by a database system which is equipped with the distributed training algorithms to support the efficient training of the large-scale DNN on computing clusters. In this paper, we propose a novel idea to implement general artificial intelligence. Eventually, the large-scale model is validated on several different modal tasks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.jos.org.cn/josen/article/abstract/6470" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/brain-inspired-large-scale-deep-neural-network-system/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jiulin-lang/">Jiulin Lang</a></span>, <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/yi-gao/">Yi Gao</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/knowledge-distillation-method-for-surface-defect-detection/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/knowledge-distillation-method-for-surface-defect-detection/">
    <div class="img-hover-zoom"><img src="/publication/knowledge-distillation-method-for-surface-defect-detection/featured_hu7a50f73cb59e59faaf44be86788b3368_114459_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Knowledge Distillation Method for Surface Defect Detection."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/knowledge-distillation-method-for-surface-defect-detection/">Knowledge Distillation Method for Surface Defect Detection.</a>
  </h3>

  
  <a href="/publication/knowledge-distillation-method-for-surface-defect-detection/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we propose a multi-scale attention mechanism-guided knowledge distillation method for surface defect detection. Enables a lighter student model to mimic the complex teacher model through the use of knowledge distillation techniques, the proposed method improves the defect detection accuracy and maintains high real-time performance, simultaneously. Specifically, we first present a multi-scale fusion-based teacher network. Owing to the fusion of two resolution scales features, the teacher network can keep high compatibility with the low-resolution student network during knowledge distillation, so as to better direct the student model. Then, in the process of knowledge distillation, attentional mechanisms were introduced with the aim of enabling the student network to more effectively mimic the foreground attention map and features of the teacher network. Finally, in order to address the imbalance of foreground and background in defect detection, we introduce a class-weighted cross entropy loss. Experiments conducted on three benchmark datasets proved the validity and efficiency of the proposed method in surface defect detection.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-92273-3_53.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/knowledge-distillation-method-for-surface-defect-detection/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yong-wang/">Yong Wang</a></span>, <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/jianwang/">JianWang</a></span>, <span ><a href="/author/yongshegsang/">YongshegSang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/cataract-detection-based-on-ocular-b-ultrasound-images-by-collaborative-monitoring-deep-learning/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/cataract-detection-based-on-ocular-b-ultrasound-images-by-collaborative-monitoring-deep-learning/">
    <div class="img-hover-zoom"><img src="/publication/cataract-detection-based-on-ocular-b-ultrasound-images-by-collaborative-monitoring-deep-learning/featured_hu48e03def82f13b5eee026048fa6bb53a_57153_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Cataract detection based on ocular B-ultrasound images by collaborative monitoring deep learning."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/cataract-detection-based-on-ocular-b-ultrasound-images-by-collaborative-monitoring-deep-learning/">Cataract detection based on ocular B-ultrasound images by collaborative monitoring deep learning.</a>
  </h3>

  
  <a href="/publication/cataract-detection-based-on-ocular-b-ultrasound-images-by-collaborative-monitoring-deep-learning/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we collect an ocular B-ultrasound image dataset and propose a Collaborative Monitoring Deep Learning (CMDL) method to detect cataract. In the ocular B-ultrasound images, there are often strong echoes near the posterior capsule and lens, and the fine-grained ocular B-ultrasound images are often accompanied by the characteristics of weak penetrating power, low contrast, narrow imaging range, and high noise. Thus, in the proposed CMDL method, we introduce an object detection network based on YOLO-v3 to detect the focus areas we need, so as to reduce the interference of noise and improve the cataract detection accuracy of our method. Considering that the B-ultrasound image dataset we collected is small-scale, we also design three feature extraction modules to avoid over-fitting of the deep neural networks. Among them, there are a depth features extraction module based on DenseNet-161, a shape features extractor based on Fourier descriptor, and a texture features extraction module based on gray-level co-occurrence matrix. Moreover, we also introduce the collaborative learning module to improve the generalization of the proposed model. Specifically, we first fuse the depth, shape, and texture features of the eyeball and lens, respectively. Then, the fused features of the eyeball and lens are concatenated as the input of collaborative network. Finally, the introduced classification loss with the aid of collaborative loss, which distinguishes whether the eyeball and lens belong to the same category, improves the classification accuracy in cataract detection. Experimental results on our collected dataset demonstrate the effectiveness of the proposed CMDL method.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705121X00184/1-s2.0-S0950705121007048/main.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/cataract-detection-based-on-ocular-b-ultrasound-images-by-collaborative-monitoring-deep-learning/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yi-gao/">Yi Gao</a></span>, <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/jiulin-lang/">Jiulin Lang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/end-to-end-edge-detection-via-improved-transformer-model/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/end-to-end-edge-detection-via-improved-transformer-model/">
    <div class="img-hover-zoom"><img src="/publication/end-to-end-edge-detection-via-improved-transformer-model/featured_hu4af2cfd5b6e2271c61d711b058649017_100288_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="End-to-end edge detection via improved transformer model."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/end-to-end-edge-detection-via-improved-transformer-model/">End-to-end edge detection via improved transformer model.</a>
  </h3>

  
  <a href="/publication/end-to-end-edge-detection-via-improved-transformer-model/" class="summary-link">
    <div class="article-style">
      <p>Recently, many efficient edge detection methods based on deep learning have emerged and made remarkable achievements. However, there are two fundamental challenges, i.e., the extraction and fusion of different scale features, as well as the sample imbalance, making the performance of edge detection need to be further promoted. In this paper, we propose an end-to-end edge detection method implemented by improved transformer model to promote edge detection by solving multi-scale fusion and sample imbalance. Specifically, based on the transformer model, we design a multi-scale edge extraction module, which utilizes pooling layer and dilated convolution with different rates and kernels, to realize multi-scale feature extraction and fusion. Moreover, we design an efficient loss function to guide the proposed method to fit the distribution of unbalanced positive and negative samples. Extensive experiments conducted on two benchmark data sets prove that the proposed method significantly outperforms state-of-the-art methods in edge detection.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-92273-3_42.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/end-to-end-edge-detection-via-improved-transformer-model/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yuanli/">YuanLi</a></span>, <span ><a href="/author/jianwang/">JianWang</a></span>, <span ><a href="/author/weiboliang/">WeiboLiang</a></span>, <span ><a href="/author/huixue/">HuiXue</a></span>, <span ><a href="/author/zhenanhe/">ZhenanHe</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/linzhang/">LinZhang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/cr-gan-automatic-craniofacial-reconstruction-for-personal-identification/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/cr-gan-automatic-craniofacial-reconstruction-for-personal-identification/">
    <div class="img-hover-zoom"><img src="/publication/cr-gan-automatic-craniofacial-reconstruction-for-personal-identification/featured_hudfd6aaa334822062a866a250bd4e7927_244053_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="CR-GAN: Automatic craniofacial reconstruction for personal identification."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/cr-gan-automatic-craniofacial-reconstruction-for-personal-identification/">CR-GAN: Automatic craniofacial reconstruction for personal identification.</a>
  </h3>

  
  <a href="/publication/cr-gan-automatic-craniofacial-reconstruction-for-personal-identification/" class="summary-link">
    <div class="article-style">
      <p>Craniofacial reconstruction is applied to identify human remains in the absence of determination data (e.g., fingerprinting, dental records, radiological materials, or DNA), by predicting the likeness of the unidentified remains based on the internal relationship between the skull and face. Conventional 3D methods are usually based on statistical models with poor capacity, which limit the description of such complex relationship. Moreover, the required high-quality data are difficult to collect. In this study, we present a novel craniofacial reconstruction paradigm that synthesize craniofacial images from 2D computed tomography scan of skull data. The key idea is to recast craniofacial reconstruction as an image translation task, with the goal of generating corresponding craniofacial images from 2D skull images. To this end, we design an automatic skull-to-face transformation system based on deep generative adversarial nets. The system was trained on 4551 paired skull-face images obtained from 1780 CT head scans of the Han Chinese population. To the best of our knowledge, this is the only database of this magnitude in the literature. Finally, to accurately evaluate the performance of the model, a face recognition task employing five existing deep learning algorithms, —FaceNet, —SphereFace, —CosFace, —ArcFace, and —MagFace, was tested on 102 reconstruction cases in a face pool composed of 1744 CT-scan face images. The experimental results demonstrate that the proposed method can be used as an effective forensic tool.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.sciencedirect.com/science/article/pii/S0031320321005768/pdfft?md5=d495544a9fb2d4498b2c2ceede57daf0&amp;pid=1-s2.0-S0031320321005768-main.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/cr-gan-automatic-craniofacial-reconstruction-for-personal-identification/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/kexin-yang/">Kexin Yang</a></span>, <span ><a href="/author/wenqianglei/">WenqiangLei</a></span>, <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/weizhenqi/">WeizhenQi</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/pos-constrained-parallel-decoding-for-non-autoregressive-generation/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/pos-constrained-parallel-decoding-for-non-autoregressive-generation/">
    <div class="img-hover-zoom"><img src="/publication/pos-constrained-parallel-decoding-for-non-autoregressive-generation/featured_hu774459e22d8aacbd85aa4acddde74e11_52442_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="POS-Constrained Parallel Decoding for Non-autoregressive Generation."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/pos-constrained-parallel-decoding-for-non-autoregressive-generation/">POS-Constrained Parallel Decoding for Non-autoregressive Generation.</a>
  </h3>

  
  <a href="/publication/pos-constrained-parallel-decoding-for-non-autoregressive-generation/" class="summary-link">
    <div class="article-style">
      <p>The multimodality problem has become a major challenge of existing non-autoregressive generation (NAG) systems. A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as “teacher AG”). The success of such methods may largely depend on a latent assumption, i.e., the teacher AG is superior to the NAG model. However, in this work, we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation. To provide a feasible solution to the multimodality problem of NAG, we propose incorporating linguistic structure (Part-of-Speech sequence in particular) into NAG inference instead of relying on teacher AG. More specifically, the proposed POS-constrained Parallel Decoding (POSPD) method aims at providing a specific POS sequence to constrain the NAG model during decoding. Our experiments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowledge distillation. This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://aclanthology.org/2021.acl-long.467.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/pos-constrained-parallel-decoding-for-non-autoregressive-generation/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/hongjie-wu/">Hongjie Wu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/jianwang/">JianWang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/automatic-cataract-detection-with-multi-task-learning/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/automatic-cataract-detection-with-multi-task-learning/">
    <div class="img-hover-zoom"><img src="/publication/automatic-cataract-detection-with-multi-task-learning/featured_hu77633a436b983e66f83a8dfbbef386a2_53357_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Automatic Cataract Detection with Multi-Task Learning."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/automatic-cataract-detection-with-multi-task-learning/">Automatic Cataract Detection with Multi-Task Learning.</a>
  </h3>

  
  <a href="/publication/automatic-cataract-detection-with-multi-task-learning/" class="summary-link">
    <div class="article-style">
      <p>Cataract is one of the most prevalent diseases among the elderly. As the population ages, the incidence of cataracts is on the rise. Early diagnosis and treatment are essential for cataracts. The routine early diagnosis relies on B-scan eye ultrasound images, developing deep learning-based automatic cataract detection makes great sense. However, ultrasound images are complex and contain irrelevant backgrounds, the lens takes up only a small part. Besides, detection networks commonly use only one label as supervision, which leads to low classification accuracy and poor generalization. This paper focuses on making the most of the information in the images, thus proposing a new paradigm for automatic cataract detection. First, an object detection network is included to locate the eyeball area and eliminate the influence of the background. Next, we construct a dataset with multiple labels for each image. We extract the text descriptions of ultrasound images into labels so that each image is tagged with multiple labels. Then we applied the multi-task learning (MTL) methods to cataract detection. The accuracy of classification is significantly improved compared to data with only one label. Last, we propose two gradient-guided auxiliary learning methods to make the auxiliary tasks improve the performance of the main task (cataract detection). The experimental results show that our proposed methods further improve the classification accuracy.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9533424" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/automatic-cataract-detection-with-multi-task-learning/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yuhaozhou/">YuhaoZhou</a></span>, <span ><a href="/author/qingye/">QingYe</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/communication-efficient-federated-learning-with-compensated-overlap-fedavg/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/communication-efficient-federated-learning-with-compensated-overlap-fedavg/">
    <div class="img-hover-zoom"><img src="/publication/communication-efficient-federated-learning-with-compensated-overlap-fedavg/featured_hu89c478a121ded365c5040d95ab33c2b0_109429_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Communication-Efficient Federated Learning With Compensated Overlap-FedAvg."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/communication-efficient-federated-learning-with-compensated-overlap-fedavg/">Communication-Efficient Federated Learning With Compensated Overlap-FedAvg.</a>
  </h3>

  
  <a href="/publication/communication-efficient-federated-learning-with-compensated-overlap-fedavg/" class="summary-link">
    <div class="article-style">
      <p>While petabytes of data are generated each day by a number of independent computing devices, only a few of them can be finally collected and used for deep learning (DL) due to the apprehension of data security and privacy leakage, thus seriously retarding the extension of DL. In such a circumstance, federated learning (FL) was proposed to perform model training by multiple clients&rsquo; combined data without the dataset sharing within the cluster. Nevertheless, federated learning with periodic model averaging (FedAvg) introduced massive communication overhead as the synchronized data in each iteration is about the same size as the model, and thereby leading to a low communication efficiency. Consequently, variant proposals focusing on the communication rounds reduction and data compression were proposed to decrease the communication overhead of FL. In this article, we propose Overlap-FedAvg, an innovative framework that loosed the chain-like constraint of federated learning and paralleled the model training phase with the model communication phase (i.e., uploading local models and downloading the global model), so that the latter phase could be totally covered by the former phase. Compared to vanilla FedAvg, Overlap-FedAvg was further developed with a hierarchical computing strategy, a data compensation mechanism, and a nesterov accelerated gradients (NAG) algorithm. In Particular, Overlap-FedAvg is orthogonal to many other compression methods so that they could be applied together to maximize the utilization of the cluster. Besides, the theoretical analysis is provided to prove the convergence of the proposed framework. Extensive experiments conducting on both image classification and natural language processing tasks with multiple models and datasets also demonstrate that the proposed framework substantially reduced the communication overhead and boosted the federated learning process.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9459540" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/communication-efficient-federated-learning-with-compensated-overlap-fedavg/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/zhenanhe/">ZhenanHe</a></span>, <span ><a href="/author/yunxiali/">YunxiaLi,</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/zero-shot-learning-via-structure-aligned-generative-adversarial-network/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/zero-shot-learning-via-structure-aligned-generative-adversarial-network/">
    <div class="img-hover-zoom"><img src="/publication/zero-shot-learning-via-structure-aligned-generative-adversarial-network/featured_huf74046d2e5e388be183b3e63ab8f823d_77106_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Zero-Shot Learning via Structure-Aligned Generative Adversarial Network."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/zero-shot-learning-via-structure-aligned-generative-adversarial-network/">Zero-Shot Learning via Structure-Aligned Generative Adversarial Network.</a>
  </h3>

  
  <a href="/publication/zero-shot-learning-via-structure-aligned-generative-adversarial-network/" class="summary-link">
    <div class="article-style">
      <p>In this article, we propose a structure-aligned generative adversarial network framework to improve zero-shot learning (ZSL) by mitigating the semantic gap, domain shift, and hubness problem. The proposed framework contains two parts, i.e., a generative adversarial network with a softmax classifier part, and a structure-aligned part. In the first part, the generative adversarial network aims at generating pseudovisual features through the guiding generator and discriminator play the minimax two-player game together. At the same time, the softmax classifier is committed to increasing the interclass distance and reducing intraclass distance. Then, the harmful effect of domain shift and hubness problems can be mitigated. In another part, we introduce a structure-aligned module where the structural consistency between visual space and semantic space is learned. By aligning the structure between visual space and semantic space, the semantic gap between them can be bridged. The performance of classification is improved when the structure-aligned visual-semantic embedding space is transferred to the unseen classes. Our framework reformulates the ZSL as a standard fully supervised classification task using the pseudovisual features of unseen classes. Extensive experiments conducted on five benchmark data sets demonstrate that the proposed framework significantly outperforms state-of-the-art methods in both conventional and generalized settings.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9449653" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/zero-shot-learning-via-structure-aligned-generative-adversarial-network/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/hang-zhang/">Hang Zhang</a></span>, <span ><a href="/author/yeyungong/">YeyunGong</a></span>, <span ><a href="/author/yelongshen/">YelongShen</a></span>, <span ><a href="/author/weishengli/">WeishengLi</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/nanduan/">NanDuan</a></span>, <span ><a href="/author/weizhuchen/">WeizhuChen</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/poolingformer-long-document-modeling-with-pooling-attention/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/poolingformer-long-document-modeling-with-pooling-attention/">
    <div class="img-hover-zoom"><img src="/publication/poolingformer-long-document-modeling-with-pooling-attention/featured_hu5f67f7b449272f8d57801224897918c9_60555_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Poolingformer: Long Document Modeling with Pooling Attention."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/poolingformer-long-document-modeling-with-pooling-attention/">Poolingformer: Long Document Modeling with Pooling Attention.</a>
  </h3>

  
  <a href="/publication/poolingformer-long-document-modeling-with-pooling-attention/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we introduce a two-level attention schema, Poolingformer, for long document modeling. Its first level uses a smaller sliding window pattern to aggregate information from neighbors. Its second level employs a larger window to increase receptive fields with pooling attention to reduce both computational cost and memory consumption. We first evaluate Poolingformer on two long sequence QA tasks: the monolingual NQ and the multilingual TyDi QA. Experimental results show that Poolingformer sits atop three official leaderboards measured by F1, outperforming previous state-of-the-art models by 1.9 points (79.8 vs. 77.9) on NQ long answer, 1.9 points (79.5 vs. 77.6) on TyDi QA passage answer, and 1.6 points (67.6 vs. 66.0) on TyDi QA minimal answer. We further evaluate Poolingformer on a long sequence summarization task. Experimental results on the arXiv benchmark continue to demonstrate its superior performance.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://proceedings.mlr.press/v139/zhang21h/zhang21h.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/poolingformer-long-document-modeling-with-pooling-attention/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jindi-lv/">Jindi Lv</a></span>, <span ><a href="/author/qingye/">QingYe</a></span>, <span ><a href="/author/yannansun/">YannanSun</a></span>, <span ><a href="/author/juan-zhao/">Juan Zhao</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/heart-darts-classification-of-heartbeats-using-differentiable-architecture-search/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/heart-darts-classification-of-heartbeats-using-differentiable-architecture-search/">
    <div class="img-hover-zoom"><img src="/publication/heart-darts-classification-of-heartbeats-using-differentiable-architecture-search/featured_hue16d45f22a04c55468244b7e5d6a2495_19811_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Heart-Darts: Classification of Heartbeats Using Differentiable Architecture Search."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/heart-darts-classification-of-heartbeats-using-differentiable-architecture-search/">Heart-Darts: Classification of Heartbeats Using Differentiable Architecture Search.</a>
  </h3>

  
  <a href="/publication/heart-darts-classification-of-heartbeats-using-differentiable-architecture-search/" class="summary-link">
    <div class="article-style">
      <p>Arrhythmia is a cardiovascular disease that manifests irregular heartbeats. In arrhythmia detection, the electrocardiogram (ECG) signal is an important diagnostic technique. However, manually evaluating ECG signals is a complicated and time-consuming task. With the application of convolutional neural networks (CNNs), the evaluation process has been accelerated and the performance is improved. It is noteworthy that the performance of CNNs heavily depends on their architecture design, which is a complex process grounded on expert experience and trial-and-error. In this paper, we propose a novel approach, Heart-Darts, to efficiently classify the ECG signals by automatically designing the CNN model with the differentiable architecture search (i.e., Darts, a cell-based neural architecture search method). Specifically, we initially search a cell architecture by Darts and then customize a novel CNN model for ECG classification based on the obtained cells. To investigate the efficiency of the proposed method, we evaluate the constructed model on the MIT-BIH arrhythmia database. Additionally, the extensibility of the proposed CNN model is validated on two other new databases. Extensive experimental results demonstrate that the proposed method outperforms several state-of-the-art CNN models in ECG classification in terms of both performance and generalization capability.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2105.00693.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/heart-darts-classification-of-heartbeats-using-differentiable-architecture-search/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yuhaozhou/">YuhaoZhou</a></span>, <span ><a href="/author/xihuali/">XihuaLi</a></span>, <span ><a href="/author/yunbocao/">YunboCao</a></span>, <span ><a href="/author/xueminzhao/">XueminZhao</a></span>, <span ><a href="/author/qingye/">QingYe</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/lana-towards-personalized-deep-knowledge-tracing-through-distinguishable-interactive-sequences/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/lana-towards-personalized-deep-knowledge-tracing-through-distinguishable-interactive-sequences/">
    <div class="img-hover-zoom"><img src="/publication/lana-towards-personalized-deep-knowledge-tracing-through-distinguishable-interactive-sequences/featured_hud5a8c7cc42cf53ccc1e5f698458baef7_81907_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="LANA: Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/lana-towards-personalized-deep-knowledge-tracing-through-distinguishable-interactive-sequences/">LANA: Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences.</a>
  </h3>

  
  <a href="/publication/lana-towards-personalized-deep-knowledge-tracing-through-distinguishable-interactive-sequences/" class="summary-link">
    <div class="article-style">
      <p>In educational applications, Knowledge Tracing (KT), the problem of accurately predicting students&rsquo; responses to future questions by summarizing their knowledge states, has been widely studied for decades as it is considered a fundamental task towards adaptive online learning. Among all the proposed KT methods, Deep Knowledge Tracing (DKT) and its variants are by far the most effective ones due to the high flexibility of the neural network. However, DKT often ignores the inherent differences between students (e.g. memory skills, reasoning skills, &hellip;), averaging the performances of all students, leading to the lack of personalization, and therefore was considered insufficient for adaptive learning. To alleviate this problem, in this paper, we proposed Leveled Attentive KNowledge TrAcing (LANA), which firstly uses a novel student-related features extractor (SRFE) to distill students&rsquo; unique inherent properties from their respective interactive sequences. Secondly, the pivot module was utilized to dynamically reconstruct the decoder of the neural network on attention of the extracted features, successfully distinguishing the performance between students over time. Moreover, inspired by Item Response Theory (IRT), the interpretable Rasch model was used to cluster students by their ability levels, and thereby utilizing leveled learning to assign different encoders to different groups of students. With pivot module reconstructed the decoder for individual students and leveled learning specialized encoders for groups, personalized DKT was achieved. Extensive experiments conducted on two real-world large-scale datasets demonstrated that our proposed LANA improves the AUC score by at least 1.00% (i.e. EdNet 1.46% and RAIEd2020 1.00%), substantially surpassing the other State-Of-The-Art KT methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2105.06266.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/lana-towards-personalized-deep-knowledge-tracing-through-distinguishable-interactive-sequences/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/huishuang-tian/">Huishuang Tian</a></span>, <span ><a href="/author/kexin-yang/">Kexin Yang</a></span>, <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/anchibert-a-pre-trained-model-for-ancient-chinese-language-understanding-and-generation/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/anchibert-a-pre-trained-model-for-ancient-chinese-language-understanding-and-generation/">
    <div class="img-hover-zoom"><img src="/publication/anchibert-a-pre-trained-model-for-ancient-chinese-language-understanding-and-generation/featured_huca38bef82c0a4ed43047f8a9804e60a2_115679_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="AnchiBERT: A Pre-Trained Model for Ancient Chinese Language Understanding and Generation."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/anchibert-a-pre-trained-model-for-ancient-chinese-language-understanding-and-generation/">AnchiBERT: A Pre-Trained Model for Ancient Chinese Language Understanding and Generation.</a>
  </h3>

  
  <a href="/publication/anchibert-a-pre-trained-model-for-ancient-chinese-language-understanding-and-generation/" class="summary-link">
    <div class="article-style">
      <p>Ancient Chinese is the essence of Chinese culture. There are several natural language processing tasks of ancient Chinese domain, such as ancient-modern Chinese translation, poem generation, and couplet generation. Previous studies usually use the supervised models which deeply rely on parallel data. However, it is difficult to obtain large-scale parallel data of ancient Chinese. In order to make full use of the more easily available monolingual ancient Chinese corpora, we release An-chiBERT, a pre-trained language model based on the architecture of BERT, which is trained on large-scale ancient Chinese corpora. We evaluate AnchiBERT on both language understanding and generation tasks, including poem classification, ancient-modern Chinese translation, poem generation, and couplet generation. The experimental results show that AnchiBERT outperforms BERT as well as the non-pretrained models and achieves state-of - the-art results in all cases.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9534342" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/anchibert-a-pre-trained-model-for-ancient-chinese-language-understanding-and-generation/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/kexin-yang/">Kexin Yang</a></span>, <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/qian-qu/">Qian Qu</a></span>, <span ><a href="/author/yongshengsang/">YongshengSang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/an-automatic-evaluation-metric-for-ancient-modern-chinese-translation/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/an-automatic-evaluation-metric-for-ancient-modern-chinese-translation/">
    <div class="img-hover-zoom"><img src="/publication/an-automatic-evaluation-metric-for-ancient-modern-chinese-translation/featured_hu452bd8009076a1f254eab36dd9714cff_54829_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="An automatic evaluation metric for Ancient-Modern Chinese translation."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/an-automatic-evaluation-metric-for-ancient-modern-chinese-translation/">An automatic evaluation metric for Ancient-Modern Chinese translation.</a>
  </h3>

  
  <a href="/publication/an-automatic-evaluation-metric-for-ancient-modern-chinese-translation/" class="summary-link">
    <div class="article-style">
      <p>As a written language used for thousands of years, Ancient Chinese has some special characteristics like complex semantics as polysemy and the one-to-many alignment with Modern Chinese. Thus it may be translated in a large number of fully different but equally correct ways. In the absence of multiple references, reference-dependent evaluations like Bilingual Evaluation Understudy (BLEU) cannot identify potentially correct translation results. The explore on automatic evaluation of Ancient-Modern Chinese Translation is completely lacking. In this paper, we proposed an automatic evaluation metric for Ancient-Modern Chinese Translation called DTE (Dual-based Translation Evaluation), which can be used to evaluate one-to-many alignment in the absence of multiple references. When using DTE to evaluate, we found that the proper nouns often could not be correctly translated. Hence, we designed a new word segmentation method to improve the translation of proper nouns without increasing the size of the model vocabulary. Experiments show that DTE outperforms several general evaluations in terms of similarity to the evaluation of human experts. Meanwhile, the new word segmentation method promotes the Ancient-Modern Chinese translation models perform better on proper nouns’ translation, and get higher scores on both BLEU and DTE.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/an-automatic-evaluation-metric-for-ancient-modern-chinese-translation/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/qingye/">QingYe</a></span>, <span ><a href="/author/yanansun/">YananSun</a></span>, <span ><a href="/author/jixinzhang/">JixinZhang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/distributed-framework-for-ea-based-nas/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/distributed-framework-for-ea-based-nas/">
    <div class="img-hover-zoom"><img src="/publication/distributed-framework-for-ea-based-nas/featured_hu08807d18289b525ac8dc5e6a5348870f_320441_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="A distributed framework for EA-based NAS."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/distributed-framework-for-ea-based-nas/">A distributed framework for EA-based NAS.</a>
  </h3>

  
  <a href="/publication/distributed-framework-for-ea-based-nas/" class="summary-link">
    <div class="article-style">
      <p>Evolutionary Algorithms(EA) are widely applied in Neural Architecture Search(NAS) and have achieved appealing results. Different EA-based NAS algorithms may utilize different encoding schemes for network representation, while they have the same workflow (i.e., the initialization of the population, individual evaluation, and evolution). Because each individual needs complete training and validation on the target dataset, the EA-based NAS always consumes significant computation and time inevitably, which results in the bottleneck of this approach. To ameliorate this issue, this paper proposes a distributed framework to boost the computing of the EA-based NAS. This framework is a server/worker model where the server distributes individuals, collects the validated individuals and hosts the evolution operations. Meanwhile, the most time-consuming phase (i.e., individual evaluation) is allocated to the computational workers. Additionally, a new packet structure of the message delivered in the cluster is designed to encapsulate various network representation of different EA-based NAS algorithms. We design an EA-based NAS algorithm as a sample to investigate the effectiveness of the proposed framework. Extensive experiments are performed on an illustrative cluster with different scales, and the results reveal that the framework can achieve a nearly linear reduction of the training time with the increase of the computational workers.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/9305984" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/distributed-framework-for-ea-based-nas/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/chanjuan-li/">Chanjuan Li</a></span>, <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/kexin-yang/">Kexin Yang</a></span>, <span ><a href="/author/xiaominghuang/">XiaomingHuang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/herb-know-knowledge-enhanced-prescription-generation-for-traditional-chinese-medicine/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/herb-know-knowledge-enhanced-prescription-generation-for-traditional-chinese-medicine/">
    <div class="img-hover-zoom"><img src="/publication/herb-know-knowledge-enhanced-prescription-generation-for-traditional-chinese-medicine/featured_hu87e78ff12aedc085d5660add928a05df_105770_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Herb-Know: Knowledge Enhanced Prescription Generation for Traditional Chinese Medicine."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/herb-know-knowledge-enhanced-prescription-generation-for-traditional-chinese-medicine/">Herb-Know: Knowledge Enhanced Prescription Generation for Traditional Chinese Medicine.</a>
  </h3>

  
  <a href="/publication/herb-know-knowledge-enhanced-prescription-generation-for-traditional-chinese-medicine/" class="summary-link">
    <div class="article-style">
      <p>Prescription generation of traditional Chinese medicine (TCM) is a meaningful and challenging problem. Previous researches mainly model the relationship between symptoms and herbal prescription directly. However, TCM practitioners often take herb effects into consideration when prescribing. Few works focus on fusing the external knowledge of herbs. In this paper, we explore how to generate a prescription with the knowledge of herb effects under the given symptoms. We propose Herb-Know, a sequence to sequence (seq2seq) model with pointer network, where the prescription is conditioned over two inputs (symptoms and pre-selected herb candidates). To the best of our knowledge, this is the first attempt to generate a prescription with a knowledge enhanced seq2seq model. The experimental results demonstrate that our method can make use of knowledge to generate informative and reasonable herbs, which outperforms other baseline models.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9313476" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/herb-know-knowledge-enhanced-prescription-generation-for-traditional-chinese-medicine/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/feifei-fu/">Feifei Fu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/mao-li/">Mao Li</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/multistyle-chinese-art-painting-generation-of-flowers/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/multistyle-chinese-art-painting-generation-of-flowers/">
    <div class="img-hover-zoom"><img src="/publication/multistyle-chinese-art-painting-generation-of-flowers/featured_hu6224c5e3531dbd7bff05446f2a970b4f_408436_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Multi‐style Chinese art painting generation of flowers."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/multistyle-chinese-art-painting-generation-of-flowers/">Multi‐style Chinese art painting generation of flowers.</a>
  </h3>

  
  <a href="/publication/multistyle-chinese-art-painting-generation-of-flowers/" class="summary-link">
    <div class="article-style">
      <p>With the proposal and development of Generative Adversarial Networks, the great achievements in the field of image generation are made. Meanwhile, many works related to the generation of painting art have also been derived. However, due to the difficulty of data collection and the fundamental challenge from freehand expressions, the generation of traditional Chinese painting is still far from being perfect. This paper specialises in Chinese art painting generation of flowers, which is important and classic, by deep learning method. First, an unpaired flowers paintings data set containing three classic Chinese painting style: line drawing, meticulous, and ink is constructed. Then, based on the collected dataset, a Flower-Generative Adversarial Network framework to generate multi-style Chinese art painting of flowers is proposed. The Flower-Generative Adversarial Network, consisting of attention-guided generators and discriminators, transfers the style among line drawing, meticulous, and ink by an adversarial training way. Moreover, in order to solve the problem of artefact and blur in image generation by existing methods, a new loss function called Multi-Scale Structural Similarity to force the structure preservation is introduced. Extensive experiments show that the proposed Flower-Generative Adversarial Network framework can produce better and multi-style Chinese art painting of flowers than existing methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/ipr2.12059" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/multistyle-chinese-art-painting-generation-of-flowers/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/chuan-liu/">Chuan Liu</a></span>, <span ><a href="/author/qingye/">QingYe</a></span>, <span ><a href="/author/xiaominghuang/">XiaomingHuang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/superconv-strengthening-the-convolution-kernel-via-weight-sharing/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/superconv-strengthening-the-convolution-kernel-via-weight-sharing/">
    <div class="img-hover-zoom"><img src="/publication/superconv-strengthening-the-convolution-kernel-via-weight-sharing/featured_hu61cfccdbb56996432b402a4107254e26_67626_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="SuperConv Strengthening the Convolution Kernel via Weight Sharing."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/superconv-strengthening-the-convolution-kernel-via-weight-sharing/">SuperConv Strengthening the Convolution Kernel via Weight Sharing.</a>
  </h3>

  
  <a href="/publication/superconv-strengthening-the-convolution-kernel-via-weight-sharing/" class="summary-link">
    <div class="article-style">
      <p>For the current neural network models, in order to improve the accuracy of the models, we need efficient plug-and-play modules. Therefore, many efficient plug-and-play operations are proposed, such as Asymmetric Convolution Block (ACB). However, the introduction of multi-branch convolution kernels in ACB increases the trainable parameters, which is an extra burden to the training of large models. In this work, SuperConv is proposed to reduce the trainable parameters while maintaining the advantages of ACB. SuperConv utilizes the method in single-path NAS to encode the convolution kernels of different sizes in multiple branches into a super-kernel, so that the convolution kernels can share some weights with each other. In addition, we introduce SuperConv into MixConv and propose SuperMixConv (SP-MixConv). To verify the effectiveness of SP-MixConv, ACB, MixConv and SP-MixConv are inserted into the Cifar-quick model and the model with SP-MixConv gets the best accuracy on CIFAR-10 and CIFAR-100. And SuperConv and SP-MixConv will not add extra burden in inference. Simultaneously, SuperConv is very easy to implement, using existing tools such as Pytorch, and is also an interesting attempt for the design of efficient plug-and-play convolution block.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.researchgate.net/publication/347056231_SuperConv_Strengthening_the_Convolution_Kernel_via_Weight_Sharing" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/superconv-strengthening-the-convolution-kernel-via-weight-sharing/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/yangzhukuang/">YangzhuKuang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/jingluhu/">JingluHu</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/san-sampling-adversarial-networks-for-zero-shot-learning/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/san-sampling-adversarial-networks-for-zero-shot-learning/">
    <div class="img-hover-zoom"><img src="/publication/san-sampling-adversarial-networks-for-zero-shot-learning/featured_hu11920c06e66481fd52d5a5190f6e966c_69749_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="SAN Sampling Adversarial Networks for Zero-Shot Learning."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/san-sampling-adversarial-networks-for-zero-shot-learning/">SAN Sampling Adversarial Networks for Zero-Shot Learning.</a>
  </h3>

  
  <a href="/publication/san-sampling-adversarial-networks-for-zero-shot-learning/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we propose a Sampling Adversarial Networks (SAN) framework to improve Zero-Shot Learning (ZSL) by mitigating the hubness and semantic gap problem. The SAN framework incorporates a sampling model and a discriminating model, and corresponds them to the minimax two-player game. Specifically, given the semantic embedding, the sampling model samples the visual features from the training set to approach the discriminator’s decision boundary. Then, the discriminator distinguishes the matching visual-semantic pairs from the sampled data. On the one hand, by the measurement of the matching degree of visual-semantic pairs and the adversarial training way, the visual-semantic embedding built by the proposed SAN decreases the intra-class distance and increases the inter-class separation. Then, the reduction of universal neighbours in the visual-semantic embedding subspace alleviates the hubness problem. On the other, the sampled rather than directly generated visual features maintain the same manifold as the real data, mitigating the semantic gap problem. Experiments show that the sampler and discriminator of the SAN framework outperform state-of-the-art methods both in conventional and generalized ZSL settings.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/TCvivi/Sampling-Adversarial-Networks-for-Zero-Shot-Learning" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/san-sampling-adversarial-networks-for-zero-shot-learning/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/mingfeng-xue/">Mingfeng Xue</a></span>, <span ><a href="/author/hang-zhang/">Hang Zhang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/key-factors-of-email-subject-generation/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/key-factors-of-email-subject-generation/">Key Factors of Email Subject Generation.</a>
  </h3>

  
  <a href="/publication/key-factors-of-email-subject-generation/" class="summary-link">
    <div class="article-style">
      <p>Automatic email subject generation is of great significance to both the recipient and the email system. The method of using deep neural network to solve the automatically generated task of email subject line has been proposed recently. We experimentally explored the performance impact of multiple elements in this task. These experimental results will provide some guiding significance for the future research of this task. As far as we know, this is the first work to study and analyze the effects of related elements.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.researchgate.net/publication/347039739_Key_Factors_of_Email_Subject_Generation" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/key-factors-of-email-subject-generation/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/liao-chen/">Liao Chen</a></span>, <span ><a href="/author/zhichen-lai/">Zhichen Lai</a></span>, <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/exploration-on-the-generation-of-chinese-palindrome-poetry/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/exploration-on-the-generation-of-chinese-palindrome-poetry/">
    <div class="img-hover-zoom"><img src="/publication/exploration-on-the-generation-of-chinese-palindrome-poetry/featured_hufccc4bddb2c88c65cd651c1a532e5e07_466318_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Exploration on the Generation of Chinese Palindrome Poetry."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/exploration-on-the-generation-of-chinese-palindrome-poetry/">Exploration on the Generation of Chinese Palindrome Poetry.</a>
  </h3>

  
  <a href="/publication/exploration-on-the-generation-of-chinese-palindrome-poetry/" class="summary-link">
    <div class="article-style">
      <p>Recently, Chinese poetry generation gains many significant achievement with the development of deep learning. However, existing methods can not generate Chinese palindrome poetry. Besides, there is no public dataset of Chinese palindrome poetry. In this paper, we propose a novel Chinese palindrome poetry generation model, named Chinese Palindrome Poetry Generation Model (CPPGM), based on the universal seq2seq model and language model with specific beam search algorithms. In addition, the proposed model is the first to generate Chinese palindrome poetry automatically, and is applicable to other palindromes, such as palindrome couplets. Compared with several methods we propose, the experimental results demonstrate the superiority of CPPGM with machine evaluation as well as human judgment.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.researchgate.net/publication/347035783_Exploration_on_the_Generation_of_Chinese_Palindrome_Poetry" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/exploration-on-the-generation-of-chinese-palindrome-poetry/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yaochen/">YaoChen</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/yanansun/">YananSun</a></span>, <span ><a href="/author/bijuejia/">BijueJia</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/heart-sound-segmentation-via-duration-longshort-term-memory-neural-network/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/heart-sound-segmentation-via-duration-longshort-term-memory-neural-network/">
    <div class="img-hover-zoom"><img src="/publication/heart-sound-segmentation-via-duration-longshort-term-memory-neural-network/featured_hue6eaa909780c4ded803cca290f1e6684_32543_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Heart sound segmentation via Duration Long–Short Term Memory neural network."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/heart-sound-segmentation-via-duration-longshort-term-memory-neural-network/">Heart sound segmentation via Duration Long–Short Term Memory neural network.</a>
  </h3>

  
  <a href="/publication/heart-sound-segmentation-via-duration-longshort-term-memory-neural-network/" class="summary-link">
    <div class="article-style">
      <p>Heart sound segmentation, which aims at detecting the first and second heart sound in phonocardiogram, is an essential step to automatically analyze heart valve diseases. Recently, the neural network-based methods have demonstrated their promising performance in segmenting the heart sound data. However, the methods also suffer from serious limitations due to the used envelope features. The reason is largely due to that the envelope features cannot effectively model the intrinsic sequential characteristic, resulting in the poor utilization of the duration information of heart cycles. In this paper, we propose a Duration Long–Short Term Memory network (Duration LSTM) to effectively address this problem by incorporating the duration features. The proposed method is investigated in the real-world phonocardiogram dataset (Massachusetts Institute of Technology heart sounds database) and compared with the two representatives of the existing state-of-the-art methods, the experimental results demonstrate that the proposed method has the promising performance on different tolerance windows. In addition, the proposed model also has some advantages in the impact of recording length and the phenomenon of the end effect.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.sciencedirect.com/science/article/pii/S1568494620304798/pdfft?md5=efd8813756e53f5903445a049b52bed1&amp;pid=1-s2.0-S1568494620304798-main.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/heart-sound-segmentation-via-duration-longshort-term-memory-neural-network/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/yeyungong/">YeyunGong</a></span>, <span ><a href="/author/jiefu/">JieFu</a></span>, <span ><a href="/author/yuyan/">YuYan</a></span>, <span ><a href="/author/jiushengchen/">JiushengChen</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/nanduan/">NanDuan</a></span>, <span ><a href="/author/mingzhou/">MingZhou</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/question-data-augmentation-with-controllable-rewriting-in-continuous-space/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/question-data-augmentation-with-controllable-rewriting-in-continuous-space/">
    <div class="img-hover-zoom"><img src="/publication/question-data-augmentation-with-controllable-rewriting-in-continuous-space/featured_hu53054181742b1cb8ed7844f45e902caa_94745_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/question-data-augmentation-with-controllable-rewriting-in-continuous-space/">Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space.</a>
  </h3>

  
  <a href="/publication/question-data-augmentation-with-controllable-rewriting-in-continuous-space/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks. We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples. CRQDA utilizes a Transformer Autoencoder to map the original discrete question into a continuous embedding space. It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization. Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data. Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://aclanthology.org/2020.emnlp-main.467.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/question-data-augmentation-with-controllable-rewriting-in-continuous-space/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jiabijue/">JiaBijue</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/xipeng/">XiPeng</a></span>, <span ><a href="/author/yaochen/">YaoChen</a></span>, <span ><a href="/author/shenglan-yang/">Shenglan Yang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/hierarchical-regulated-iterative-network/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/hierarchical-regulated-iterative-network/">
    <div class="img-hover-zoom"><img src="/publication/hierarchical-regulated-iterative-network/featured_hu0c725bf57ece83f021949ee94fa293e5_55848_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="Hierarchical Regulated Iterative Network for Joint Task of Music Detection and Music Relative Loudness Estimation."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hierarchical-regulated-iterative-network/">Hierarchical Regulated Iterative Network for Joint Task of Music Detection and Music Relative Loudness Estimation.</a>
  </h3>

  
  <a href="/publication/hierarchical-regulated-iterative-network/" class="summary-link">
    <div class="article-style">
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9222227" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hierarchical-regulated-iterative-network/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yuhaozhou/">YuhaoZhou</a></span>, <span ><a href="/author/qingye/">QingYe</a></span>, <span ><a href="/author/hailunzhang/">HailunZhang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/hierarchical-parallel-sgd-with-stale-gradients-featuring/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/hierarchical-parallel-sgd-with-stale-gradients-featuring/">
    <div class="img-hover-zoom"><img src="/publication/hierarchical-parallel-sgd-with-stale-gradients-featuring/featured_hud3bef5d9ab2e68e6bdb781498838fe10_105496_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Hierarchical Parallel SGD with Stale Gradients Featuring."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hierarchical-parallel-sgd-with-stale-gradients-featuring/">Hierarchical Parallel SGD with Stale Gradients Featuring.</a>
  </h3>

  
  <a href="/publication/hierarchical-parallel-sgd-with-stale-gradients-featuring/" class="summary-link">
    <div class="article-style">
      <p>While distributed training significantly speeds up the training process of the deep neural network (DNN), the utilization of the cluster is relatively low due to the time-consuming data synchronizing between workers. To alleviate this problem, a novel Hierarchical Parallel SGD (HPSGD) strategy is proposed based on the observation that the data synchronization phase can be paralleled with the local training phase (i.e., Feed-forward and back-propagation). Furthermore, an improved model updating method is unitized to remedy the introduced stale gradients problem, which commits updates to the replica (i.e., a temporary model that has the same parameters as the global model) and then merges the average changes to the global model. Extensive experiments are conducted to demonstrate that the proposed HPSGD approach substantially boosts the distributed DNN training, reduces the disturbance of the stale gradients and achieves better accuracy in given fixed wall-time.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2009.02701.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hierarchical-parallel-sgd-with-stale-gradients-featuring/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/jiefu/">JieFu</a></span>, <span ><a href="/author/yidanzhang/">YidanZhang</a></span>, <span ><a href="/author/chris-pal/">Chris Pal</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/revision-in-continuous-space/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/revision-in-continuous-space/">
    <div class="img-hover-zoom"><img src="/publication/revision-in-continuous-space/featured_hu0c725bf57ece83f021949ee94fa293e5_49277_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="Revision in Continuous Space: Unsupervised Text Style Transfer without Adversarial Learning"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/revision-in-continuous-space/">Revision in Continuous Space: Unsupervised Text Style Transfer without Adversarial Learning</a>
  </h3>

  
  <a href="/publication/revision-in-continuous-space/" class="summary-link">
    <div class="article-style">
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://arxiv.org/pdf/1512.04133v1" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/revision-in-continuous-space/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/dayihengliu/Fine-Grained-Style-Transfer" target="_blank" rel="noopener">
  Code
</a>







  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/xiaofeizhang/">XiaofeiZhang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/hengzheng/">HengZheng</a></span>, <span ><a href="/author/yongshengsang/">YongshengSang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/attention-based-multi-model-ensemble-for-automatic-cataract-detection-in-b-scan-eye-ultrasound-images/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/attention-based-multi-model-ensemble-for-automatic-cataract-detection-in-b-scan-eye-ultrasound-images/">
    <div class="img-hover-zoom"><img src="/publication/attention-based-multi-model-ensemble-for-automatic-cataract-detection-in-b-scan-eye-ultrasound-images/featured_hu04a9a581d265894d2214cd293ae3228e_65896_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Attention-Based Multi-Model Ensemble for Automatic Cataract Detection in B-Scan Eye Ultrasound Images."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/attention-based-multi-model-ensemble-for-automatic-cataract-detection-in-b-scan-eye-ultrasound-images/">Attention-Based Multi-Model Ensemble for Automatic Cataract Detection in B-Scan Eye Ultrasound Images.</a>
  </h3>

  
  <a href="/publication/attention-based-multi-model-ensemble-for-automatic-cataract-detection-in-b-scan-eye-ultrasound-images/" class="summary-link">
    <div class="article-style">
      <p>Accurate detection of early-stage cataract is essential for preventing blindness, but clinical cataract diagnosis requires the professional knowledge of experienced ophthalmologists, which may present difficulties for cataract patients in poverty-stricken areas. Deep learning method has been successful in many image classification tasks, but there are still huge challenges in the field of automatic cataract detection due to two characteristics of cataract and its B-scan eye ultrasound images. First, cataract is a disease that occurs in the lens of the eyeball, but the eyeball occupies only a small part of the eye B-ultrasound image. Second, lens lesions in eye B-ultrasound images are diverse, resulting in small difference and high similarity between positive and negative samples. In this paper, we propose a multi-model ensemble method based on residual attention for cataract classification. The proposed model consists of an object detection network, three pre-trained classification networks: DenseNet-161, ResNet-152 and ResNet-101, and a model ensemble module. Each classification network incorporates a residual attention module. Experimental results on the benchmark B-scan eye ultrasound dataset show that our method can adaptively focus on the discriminative areas of cataract in the eyeball and achieves an accuracy of 97.5%, which is markedly superior to the five baseline methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9207696" target="_blank" rel="noopener">
  PDF
</a>







  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zhichen-lai/">Zhichen Lai</a></span>, <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/arbitrary-chinese-font-generation-from-a-single-reference/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/arbitrary-chinese-font-generation-from-a-single-reference/">
    <div class="img-hover-zoom"><img src="/publication/arbitrary-chinese-font-generation-from-a-single-reference/featured_hud6cdbe6ea40f4fbaf7b25b64b2908ab4_63560_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Arbitrary Chinese Font Generation from a Single Reference."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/arbitrary-chinese-font-generation-from-a-single-reference/">Arbitrary Chinese Font Generation from a Single Reference.</a>
  </h3>

  
  <a href="/publication/arbitrary-chinese-font-generation-from-a-single-reference/" class="summary-link">
    <div class="article-style">
      <p>Generating a new Chinese font from a multitude of references is an easy task, while it is quite difficult to generate it from a few references. In this paper, we investigate the problem of arbitrary Chinese font generation from a single reference and propose a deep learning based model, named One-reference Chinese Font Generation Network (OCFGNet), to automatically generate any arbitrary Chinese font from a single reference. Based on the disentangled representation learning, we separate the representations of stylized Chinese characters into style and content representations. Then we design a neural network consisting of the style encoder, the content encoder and the joint decoder for the proposed model. The style encoder extracts the style features of style references and maps them onto a continuous Variational Auto-Encoder (VAE) latent variable space while the content encoder extracts the content features of content references and maps them to the content representations. Finally, the joint decoder concatenates both representations in layer-wise to generate the character which has the style of style reference and the content of content reference. In addition, based on Generative Adversarial Network (GAN) structure, we adopt a patch-level discriminator to distinguish whether the received character is real or fake. Besides the adversarial loss, we not only adopt L1-regularized per-pix loss, but also combine a novel loss term Structural SIMilarity (SSIM) together to further drive our model to generate clear and satisfactory results. The experimental results demonstrate that the proposed model can not only extract style and content features well, but also have good performance in the generation of Chinese fonts from a single reference.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9206919" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/arbitrary-chinese-font-generation-from-a-single-reference/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/qingye/">QingYe</a></span>, <span ><a href="/author/yuxuanhan/">YuxuanHan</a></span>, <span ><a href="/author/yanansun/">YananSun</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/pso-psparameter-synchronization-with-particle-swarm-optimization-for-distributed-training-of-deep-neural-networks/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/pso-psparameter-synchronization-with-particle-swarm-optimization-for-distributed-training-of-deep-neural-networks/">
    <div class="img-hover-zoom"><img src="/publication/pso-psparameter-synchronization-with-particle-swarm-optimization-for-distributed-training-of-deep-neural-networks/featured_hu61bf4e2a0d817099a7519c2e9bb52834_71262_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="PSO-PSParameter Synchronization with Particle Swarm Optimization for Distributed Training of Deep Neural Networks."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/pso-psparameter-synchronization-with-particle-swarm-optimization-for-distributed-training-of-deep-neural-networks/">PSO-PSParameter Synchronization with Particle Swarm Optimization for Distributed Training of Deep Neural Networks.</a>
  </h3>

  
  <a href="/publication/pso-psparameter-synchronization-with-particle-swarm-optimization-for-distributed-training-of-deep-neural-networks/" class="summary-link">
    <div class="article-style">
      <p>Parameter updating is an important stage in parallelism-based distributed deep learning. Synchronous methods are widely used in distributed training the Deep Neural Networks (DNNs). To reduce the communication and synchronization overhead of synchronous methods, decreasing the synchronization frequency (e.g., every n mini-batches) is a straightforward approach. However, it often suffers from poor convergence. In this paper, we propose a new algorithm of integrating Particle Swarm Optimization (PSO) into the distributed training process of DNNs to automatically compute new parameters. In the proposed algorithm, a computing work is encoded by a particle, the weights of DNNs and the training loss are modeled by the particle attributes. At each synchronization stage, the weights are updated by PSO from the sub weights gathered from all workers, instead of averaging the weights or the gradients. To verify the performance of the proposed algorithm, the experiments are performed on two commonly used image classification benchmarks: MNIST and CIFAR10, and compared with the peer competitors at multiple different synchronization configurations. The experimental results demonstrate the competitiveness of the proposed algorithm.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9207698" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/pso-psparameter-synchronization-with-particle-swarm-optimization-for-distributed-training-of-deep-neural-networks/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dongbo-liu/">Dongbo Liu</a></span>, <span ><a href="/author/zhenanhe/">ZhenanHe</a></span>, <span class="author-highlighted"><a href="/author/dongdongchen/">DongdongChen</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/an-improved-dual-channel-network-to-eliminate-catastrophic-forgetting/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/an-improved-dual-channel-network-to-eliminate-catastrophic-forgetting/">
    <div class="img-hover-zoom"><img src="/publication/an-improved-dual-channel-network-to-eliminate-catastrophic-forgetting/featured_hu84294cb22a16231af1e21e12b2656837_191727_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="An Improved Dual-Channel Network to Eliminate Catastrophic Forgetting."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/an-improved-dual-channel-network-to-eliminate-catastrophic-forgetting/">An Improved Dual-Channel Network to Eliminate Catastrophic Forgetting.</a>
  </h3>

  
  <a href="/publication/an-improved-dual-channel-network-to-eliminate-catastrophic-forgetting/" class="summary-link">
    <div class="article-style">
      <p>Catastrophic forgetting is a chronic problem during the online training process of deep neural networks. That is, once a new data set is used to train an existing neural network, the network will lose the ability to recognize the original data set. In literature, online contrastive divergence (CD) with generative replay (GR) exploits the generative capacity of the neural network to facilitate online training. It greatly alleviates catastrophic forgetting but cannot totally eliminate it. To overcome this shortcoming and further solve the challenging issue, in this article, we propose a novel approach named asynchronous dual-channel online restricted Boltzmann machine, where online CD with dual-channel GR plays an important role in further eliminating catastrophic forgetting. The asynchronous gradient estimation, by which the Markov chain sampling and the network calculation are conducted asynchronously on separate computing nodes, is designed to speed up training. The experimental results show that the proposed method outperforms several algorithms in increasing training speed and minimizing catastrophic forgetting. Besides, online learning with dual-channel can be effectively extended to other online learning neural networks with GR and has achieved excellent results in our verification experiments.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9118953" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/an-improved-dual-channel-network-to-eliminate-catastrophic-forgetting/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jianwang/">JianWang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/xueyang/">XueYang</a></span>, <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/xipeng/">XiPeng</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/multimodal-image-to-image-translation-between-domains-with-high-internal-variability/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/multimodal-image-to-image-translation-between-domains-with-high-internal-variability/">
    <div class="img-hover-zoom"><img src="/publication/multimodal-image-to-image-translation-between-domains-with-high-internal-variability/featured_hu1472fd0413add3b691ee721fd599f43b_170079_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Multimodal Image-to-Image Translation between Domains with High Internal Variability."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/multimodal-image-to-image-translation-between-domains-with-high-internal-variability/">Multimodal Image-to-Image Translation between Domains with High Internal Variability.</a>
  </h3>

  
  <a href="/publication/multimodal-image-to-image-translation-between-domains-with-high-internal-variability/" class="summary-link">
    <div class="article-style">
      <p>Multimodal image-to-image translation based on generative adversarial networks (GANs) shows suboptimal performance in the visual domains with high internal variability, e.g., translation from multiple breeds of cats to multiple breeds of dogs. To alleviate this problem, we recast the training procedure as modeling distinct distributions which are observed sequentially, for example, when different classes are encountered over time. As a result, the discriminator may forget about the previous target distributions, known as catastrophic forgetting, leading to non-/slow convergence. Through experimental observation, we found that the discriminator does not always forget the previously learned distributions during training. Therefore, we propose a novel generator regulating GAN (GR-GAN). The proposed method encourages the discriminator to teach the generator more effectively when it remembers more of the previously learned distributions, while discouraging the discriminator to guide the generator when catastrophic forgetting happens on the discriminator. Both qualitative and quantitative results show that the proposed method is significantly superior to the state-of-the-art methods in handling the image data that are with high variability.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://link.springer.com/content/pdf/10.1007/s00500-020-05073-6.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/multimodal-image-to-image-translation-between-domains-with-high-internal-variability/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yusen-liu/">Yusen Liu</a></span>, <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/yongshengsang/">YongshengSang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/generating-chinese-poetry-from-images-via-concrete-and-abstract-information/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/generating-chinese-poetry-from-images-via-concrete-and-abstract-information/">
    <div class="img-hover-zoom"><img src="/publication/generating-chinese-poetry-from-images-via-concrete-and-abstract-information/featured_huc2cad1071974237cd168d44c3d3bb730_202653_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Generating Chinese Poetry from Images via Concrete and Abstract Information."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/generating-chinese-poetry-from-images-via-concrete-and-abstract-information/">Generating Chinese Poetry from Images via Concrete and Abstract Information.</a>
  </h3>

  
  <a href="/publication/generating-chinese-poetry-from-images-via-concrete-and-abstract-information/" class="summary-link">
    <div class="article-style">
      <p>In recent years, the automatic generation of classical Chinese poetry has made great progress. Besides focusing on improving the quality of the generated poetry, there is a new topic about generating poetry from an image. However, the existing methods for this topic still have the problem of topic drift and semantic inconsistency, and the image-poem pairs dataset is hard to be built when training these models. In this paper, we extract and integrate the Concrete and Abstract information from images to address those issues. We proposed an infilling-based Chinese poetry generation model which can infill the Concrete keywords into each line of poems in an explicit way, and an abstract information embedding to integrate the Abstract information into generated poems. In addition, we use non-parallel data during training and construct separate image datasets and poem datasets to train the different components in our framework. Both automatic and human evaluation results show that our approach can generate poems which have better consistency with images without losing the quality.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2003.10773.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/generating-chinese-poetry-from-images-via-concrete-and-abstract-information/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/yeyungong/">YeyunGong</a></span>, <span ><a href="/author/jiefu/">JieFu</a></span>, <span ><a href="/author/yuyan/">YuYan</a></span>, <span ><a href="/author/jiushengchen/">JiushengChen</a></span>, <span ><a href="/author/daxinjiang/">DaxinJiang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/nanduan/">NanDuan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/rikinet/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/rikinet/">
    <div class="img-hover-zoom"><img src="/publication/rikinet/featured_hub8d24d22855b5119a8731ae8081b7661_184499_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="RikiNet: Reading Wikipedia Pages for Natural Question Answering."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/rikinet/">RikiNet: Reading Wikipedia Pages for Natural Question Answering.</a>
  </h3>

  
  <a href="/publication/rikinet/" class="summary-link">
    <div class="article-style">
      <p>Reading   long   documents   to   answer   open-domain questions remains challenging in nat-ural language understanding. In this paper, weintroduce a new model, called RikiNet, whichreads Wikipedia pages for natural question an-swering.    RikiNet  contains  a  dynamic  para-graph  dual-attention  reader  and  a  multi-levelcascaded answer predictor. The reader dynam-ically  represents  the  document  and  questionby utilizing a set of complementary attentionmechanisms.  The representations are then fedinto the predictor to obtain the span of the shortanswer, the paragraph of the long answer, andthe  answer  type  in  a  cascaded  manner.   Onthe  Natural  Questions  (NQ)  dataset,  a  singleRikiNet achieves 74.3 F1 and 57.9 F1 on long-answer  and  short-answer  tasks.   To  our  bestknowledge, it is the first single model that out-performs the single human performance.  Fur-thermore,  an  ensemble  RikiNet  obtains  76.1F1  and  61.3  F1  on  long-answer  and  short-answer tasks, achieving the best performanceon the official NQ leaderboard</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.aclweb.org/anthology/2020.acl-main.604.pdf" target="_blank" rel="noopener">
  PDF
</a>







  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/xue-yang/">Xue Yang</a></span>, <span ><a href="/author/feng-he/">Feng He</a></span>, <span ><a href="/author/yuanyuan-chen/">Yuanyuan Chen</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/training-variational-recurrent-autoencoders-for-text-generation/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/training-variational-recurrent-autoencoders-for-text-generation/">
    <div class="img-hover-zoom"><img src="/publication/training-variational-recurrent-autoencoders-for-text-generation/featured_hu0c725bf57ece83f021949ee94fa293e5_83288_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="µ-Forcing:Training Variational Recurrent Autoencoders for Text Generation"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/training-variational-recurrent-autoencoders-for-text-generation/">µ-Forcing:Training Variational Recurrent Autoencoders for Text Generation</a>
  </h3>

  
  <a href="/publication/training-variational-recurrent-autoencoders-for-text-generation/" class="summary-link">
    <div class="article-style">
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://dl.acm.org/doi/pdf/10.1145/3341110" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/training-variational-recurrent-autoencoders-for-text-generation/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/dayihengliu/Mu-Forcing-VRAE" target="_blank" rel="noopener">
  Code
</a>




  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/d.chen/">D.Chen</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/z.yi/">Z.Yi</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/unsupervised-multi-manifold-clustering-by-learning-deep-representation/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/unsupervised-multi-manifold-clustering-by-learning-deep-representation/">
    <div class="img-hover-zoom"><img src="/publication/unsupervised-multi-manifold-clustering-by-learning-deep-representation/featured_huc697cf88965e336d2bafb07e9afbb69e_85290_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Unsupervised Multi-Manifold Clustering by Learning Deep Representation."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/unsupervised-multi-manifold-clustering-by-learning-deep-representation/">Unsupervised Multi-Manifold Clustering by Learning Deep Representation.</a>
  </h3>

  
  <a href="/publication/unsupervised-multi-manifold-clustering-by-learning-deep-representation/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we propose a novel deep manifold clustering(DMC) method for learning effective deep representationsand partitioning a dataset into clusters where each clustercontains data points from a single nonlinear manifold. Dif-ferent from other previous research efforts, we adopt deepneural network to classify and parameterize unlabeled datawhich lie on multiple manifolds. Firstly, motivated by theobservation that nearby points lie on the local of manifoldshould possess similar representations, a locality preserv-ing objective is defined to iteratively explore data relationand learn structure preserving representations. Secondly, byfinding the corresponding cluster centers from the represen-tations, a clustering-oriented objective is then proposed toguide the model to extract both discriminative and cluster-specific representations. Finally, by integrating two objectivesinto a single model with a unified cost function and opti-mizing it by using back propagation, we can obtain not onlymore powerful representations, but also more precise clustersof data. In addition, our model can be intuitively extendedto cluster out-of-sample datum. The experimental results andcomparisons with existing state-of-the-art methods show thatthe proposed method consistently achieves the best perfor-mance on various benchmark datasets</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.aaai.org/ocs/index.php/WS/AAAIW17/paper/download/15099/14689" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/unsupervised-multi-manifold-clustering-by-learning-deep-representation/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/xiaojieli/">XiaojieLi</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/zhangyi/">ZhangYi</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/manifold-alignment-based-on-sparse-local-structure-of-more-corresponding-pairs/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/manifold-alignment-based-on-sparse-local-structure-of-more-corresponding-pairs/">
    <div class="img-hover-zoom"><img src="/publication/manifold-alignment-based-on-sparse-local-structure-of-more-corresponding-pairs/featured_hu2a3fcd5d5fa21edea6a3f66818692052_167456_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Manifold Alignment Based on Sparse LocalStructures of More Corresponding Pairs."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/manifold-alignment-based-on-sparse-local-structure-of-more-corresponding-pairs/">Manifold Alignment Based on Sparse LocalStructures of More Corresponding Pairs.</a>
  </h3>

  
  <a href="/publication/manifold-alignment-based-on-sparse-local-structure-of-more-corresponding-pairs/" class="summary-link">
    <div class="article-style">
      <p>Manifold alignment is to extract the shared latentsemantic  structure  from  multiple  manifolds.   Thejoint  adjacency  matrix  plays  a  key  role  in  mani-fold alignment.  To construct the matrix, it is cru-cial  to  get  more  corresponding  pairs.This  pa-per  proposes  an  approach  to  obtain  more  and  re-liable corresponding pairs in terms of local struc-ture  correspondence.The  sparse  reconstructionweight  matrix  of  each  manifold  is  established  topreserve  the  local  geometry  of  the  original  dataset.  The sparse correspondence matrices are con-structed using the sparse local structures of corre-sponding pairs across manifolds.  Further more, anew energy function for manifold alignment is pro-posed to simultaneously match the correspondinginstances and preserve the local geometry of eachmanifold. The shared low dimensional embedding,which  provides  better  descriptions  for  the  intrin-sic geometry and relations between different man-ifolds, can be obtained by solving the optimizationproblem  with  closed-form  solution.   Experimentsdemonstrate the effectiveness of the proposed algo-rithm.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/viewPDFInterstitial/6786/7171" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/manifold-alignment-based-on-sparse-local-structure-of-more-corresponding-pairs/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/yeyungong/">YeyunGong</a></span>, <span ><a href="/author/yuyan/">YuYan</a></span>, <span ><a href="/author/jiefu/">JieFu</a></span>, <span ><a href="/author/boshao/">BoShao</a></span>, <span ><a href="/author/daxinjiang/">DaxinJiang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/nanduan/">NanDuan</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/diverse-controllable-and-keyphrase-aware-a-corpus-and-method-for-news-multi-headline-generation/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/diverse-controllable-and-keyphrase-aware-a-corpus-and-method-for-news-multi-headline-generation/">
    <div class="img-hover-zoom"><img src="/publication/diverse-controllable-and-keyphrase-aware-a-corpus-and-method-for-news-multi-headline-generation/featured_hu0397e1e98615761c6c9e2b7e173b48ab_111687_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/diverse-controllable-and-keyphrase-aware-a-corpus-and-method-for-news-multi-headline-generation/">Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation.</a>
  </h3>

  
  <a href="/publication/diverse-controllable-and-keyphrase-aware-a-corpus-and-method-for-news-multi-headline-generation/" class="summary-link">
    <div class="article-style">
      <p>News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of &lt;news article, headline, keyphrase&gt;. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://aclanthology.org/2020.emnlp-main.505.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/diverse-controllable-and-keyphrase-aware-a-corpus-and-method-for-news-multi-headline-generation/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/jiefu/">JieFu</a></span>, <span ><a href="/author/qian-qu/">Qian Qu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/bfgan/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/bfgan/">
    <div class="img-hover-zoom"><img src="/publication/bfgan/featured_hua6407b5565ab4301d5ce260e4711bfa0_94053_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="BFGAN: Backward and Forward Generative Adversarial Networks for Lexically Constrained Sentence Generation"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/bfgan/">BFGAN: Backward and Forward Generative Adversarial Networks for Lexically Constrained Sentence Generation</a>
  </h3>

  
  <a href="/publication/bfgan/" class="summary-link">
    <div class="article-style">
      <p>Incorporating prior knowledge like lexical constraints into the model&rsquo;s output to generate meaningful and coherent sentences has many applications in dialogue system, machine translation, image captioning, etc. However, existing auto-regressive models incrementally generate sentences from left to right via beam search, which makes it difficult to directly introduce lexical constraints into the generated sentences. In this paper, we propose a new algorithmic framework, dubbed BFGAN, to address this challenge. Specifically, we employ a backward generator and a forward generator to generate lexically constrained sentences together, and use a discriminator to guide the joint training of two generators by assigning them reward signals. Due to the difficulty of BFGAN training, we propose several training techniques to make the training process more stable and efficient. Our extensive experiments on three large-scale datasets with human evaluation demonstrate that BFGAN has significant improvements over previous methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/8846084" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/bfgan/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dongbo-liu/">Dongbo Liu</a></span>, <span ><a href="/author/zhenanhe/">ZhenanHe</a></span>, <span class="author-highlighted"><a href="/author/dongdongchen/">DongdongChen</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/a-network-framework-for-small-sample-learning/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/a-network-framework-for-small-sample-learning/">
    <div class="img-hover-zoom"><img src="/publication/a-network-framework-for-small-sample-learning/featured_hud6c002c43b6828fceba33344264ad063_415694_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="A Network Framework For Small-Sample Learning."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/a-network-framework-for-small-sample-learning/">A Network Framework For Small-Sample Learning.</a>
  </h3>

  
  <a href="/publication/a-network-framework-for-small-sample-learning/" class="summary-link">
    <div class="article-style">
      <p>Small-sample learning involves training a neural network on a small-sample data set. An expansion of the training set is a common way to improve the performance of neural networks in small-sample learning tasks. However, improper constraints in expanding training data will reduce the performance of the neural networks. In this article, we present certain conditions for incorporation of additional training data. According to these conditions, we propose a neural network framework for self-training using self-generated data called small-sample learning network (SSLN). The SSLN consists of two parts: the expression learning network and the sample recall generative network, both of which are constructed based on restricted Boltzmann machine (RBM). We show that this SSLN can converge as well as the RBM. Moreover, the experiment results on MNIST Digit, SVHN, CIFAR10, and STL-10 data sets reveal the superiority of the SSLN over other models.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/8931022" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/a-network-framework-for-small-sample-learning/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/d.-chen/">D. Chen,</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/z.yi/">Z.Yi</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/a-local-non-negative-pursuit-method-for-intrinsic-manifold-structure-preservation/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/a-local-non-negative-pursuit-method-for-intrinsic-manifold-structure-preservation/">
    <div class="img-hover-zoom"><img src="/publication/a-local-non-negative-pursuit-method-for-intrinsic-manifold-structure-preservation/featured_huc21e49649ece7b981bca034b3dcfa4a1_200016_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="A local non-negative pursuit method for intrinsic manifold structure preservation."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/a-local-non-negative-pursuit-method-for-intrinsic-manifold-structure-preservation/">A local non-negative pursuit method for intrinsic manifold structure preservation.</a>
  </h3>

  
  <a href="/publication/a-local-non-negative-pursuit-method-for-intrinsic-manifold-structure-preservation/" class="summary-link">
    <div class="article-style">
      <p>The local neighborhood selection plays a crucial role for most representation based manifold learning algorithms. This paper reveals that an improper selection of neighborhood for learning representation will introduce negative components in the learnt representations. Importantly, the representations with negative components will affect the intrinsic manifold structure preservation. In this paper, a local non-negative pursuit (LNP) method is proposed for neighborhood selection and non-negative representations are learnt. Moreover, it is proved that the learnt representations are sparse and convex. Theoretical analysis and experimental results show that the proposed method achieves or outperforms the state-of-the-art results on various manifold learning problems</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8203/8804" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/a-local-non-negative-pursuit-method-for-intrinsic-manifold-structure-preservation/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/xueyang/">XueYang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>, <span ><a href="/author/zhenanhe/">ZhenanHe</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/zero-shot-learning-by-mutual-information-estimation-and-maximization/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/zero-shot-learning-by-mutual-information-estimation-and-maximization/">
    <div class="img-hover-zoom"><img src="/publication/zero-shot-learning-by-mutual-information-estimation-and-maximization/featured_hu523223f944d73d0d6b8a9f317412d168_76801_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Zero-Shot Learning by Mutual Information Estimation and Maximization."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/zero-shot-learning-by-mutual-information-estimation-and-maximization/">Zero-Shot Learning by Mutual Information Estimation and Maximization.</a>
  </h3>

  
  <a href="/publication/zero-shot-learning-by-mutual-information-estimation-and-maximization/" class="summary-link">
    <div class="article-style">
      <p>The key of zero-shot learning is to use the visual-semantic embedding to transfer the knowledge from seen classes to unseen classes. In this paper, we propose to build the visual-semantic embedding by maximizing the mutual information between visual features and corresponding attributes. Then, the mutual information between visual and semantic features can be utilized to guide the knowledge transfer from seen domain to unseen domain. Since we are primarily interested in maximizing mutual information, we introduce the noise-contrastive estimation to calculate lower-bound value of mutual information. Through the noise-contrastive estimation, we reformulate zero-shot learning as a binary classification problem, i.e., classifying the matching visual-semantic pairs (positive samples) and mismatching visual-semantic pairs (negative/noise samples). Experiments conducted on five datasets demonstrate that the proposed mutual information estimators outperforms current state-of-the-art methods both in conventional and generalized zero-shot learning settings.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705120X00074/1-s2.0-S0950705120300101/main.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/zero-shot-learning-by-mutual-information-estimation-and-maximization/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yusen-liu/">Yusen Liu</a></span>, <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/deep-poetry-a-chinese-classical-poetry-generation-system/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/deep-poetry-a-chinese-classical-poetry-generation-system/">
    <div class="img-hover-zoom"><img src="/publication/deep-poetry-a-chinese-classical-poetry-generation-system/featured_hu533cbd00fab09ece2ae8253eacec0a44_111598_918x517_fill_q90_lanczos_smart1_3.png" class="article-banner" alt="Deep Poetry: A Chinese Classical Poetry Generation System."></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/deep-poetry-a-chinese-classical-poetry-generation-system/">Deep Poetry: A Chinese Classical Poetry Generation System.</a>
  </h3>

  
  <a href="/publication/deep-poetry-a-chinese-classical-poetry-generation-system/" class="summary-link">
    <div class="article-style">
      <p>In this work, we demonstrate a Chinese classical poetry generation system called Deep Poetry. Existing systems for Chinese classical poetry generation are mostly template-based and very few of them can accept multi-modal input. Unlike previous systems, Deep Poetry uses neural networks that are trained on over 200 thousand poems and 3 million ancient Chinese prose. Our system can accept plain text, images or artistic conceptions as inputs to generate Chinese classical poetry. More importantly, users are allowed to participate in the process of writing poetry by our system. For the user&rsquo;s convenience, we deploy the system at the WeChat applet platform, users can use the system on the mobile device whenever and wherever possible.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/1911.08212.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/deep-poetry-a-chinese-classical-poetry-generation-system/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/jiefu/">JieFu</a></span>, <span ><a href="/author/pengfeiliu/">PengfeiLiu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/an-inference-algorithm/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/an-inference-algorithm/">
    <div class="img-hover-zoom"><img src="/publication/an-inference-algorithm/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="An Inference Algorithm for Text Infilling with Gradient Search"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/an-inference-algorithm/">An Inference Algorithm for Text Infilling with Gradient Search</a>
  </h3>

  
  <a href="/publication/an-inference-algorithm/" class="summary-link">
    <div class="article-style">
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://eprints.soton.ac.uk/352095/1/Cushen-IMV2013.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/an-inference-algorithm/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/dayihengliu/Text-Infilling-Gradient-Search" target="_blank" rel="noopener">
  Code
</a>




  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/kexin-yang/">Kexin Yang</a></span>, <span ><a href="/author/qian-qu/">Qian Qu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/ancient-modern-chinese-translation-with-a-new-large-training-dataset/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ancient-modern-chinese-translation-with-a-new-large-training-dataset/">
    <div class="img-hover-zoom"><img src="/publication/ancient-modern-chinese-translation-with-a-new-large-training-dataset/featured_hu0c725bf57ece83f021949ee94fa293e5_73307_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="Ancient-Modern Chinese Translation with a New Large Training Dataset"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ancient-modern-chinese-translation-with-a-new-large-training-dataset/">Ancient-Modern Chinese Translation with a New Large Training Dataset</a>
  </h3>

  
  <a href="/publication/ancient-modern-chinese-translation-with-a-new-large-training-dataset/" class="summary-link">
    <div class="article-style">
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://dl.acm.org/doi/pdf/10.1145/3325887" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ancient-modern-chinese-translation-with-a-new-large-training-dataset/cite.bib">
  Cite
</button>





  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2017">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/dongdongchen/">DongdongChen</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%BB%93%E6%9E%84%E4%BF%9D%E6%8C%81%E7%A0%94%E7%A9%B6/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%BB%93%E6%9E%84%E4%BF%9D%E6%8C%81%E7%A0%94%E7%A9%B6/">
    <div class="img-hover-zoom"><img src="/publication/%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%BB%93%E6%9E%84%E4%BF%9D%E6%8C%81%E7%A0%94%E7%A9%B6/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="表达学习中结构保持研究"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%BB%93%E6%9E%84%E4%BF%9D%E6%8C%81%E7%A0%94%E7%A9%B6/">表达学习中结构保持研究</a>
  </h3>

  
  <a href="/publication/%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%BB%93%E6%9E%84%E4%BF%9D%E6%8C%81%E7%A0%94%E7%A9%B6/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/chenwei-tang/">Chenwei Tang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/">
    <div class="img-hover-zoom"><img src="/publication/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="零样本图像分类的神经网络方法"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/">零样本图像分类的神经网络方法</a>
  </h3>

  
  <a href="/publication/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/mao-li/">Mao Li</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E7%BB%98%E7%94%BB%E4%BD%9C%E5%93%81%E5%AE%A1%E7%BE%8E%E8%AE%A1%E7%AE%97%E4%B8%8E%E7%AE%97%E6%B3%95%E7%BB%98%E7%94%BB/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E7%BB%98%E7%94%BB%E4%BD%9C%E5%93%81%E5%AE%A1%E7%BE%8E%E8%AE%A1%E7%AE%97%E4%B8%8E%E7%AE%97%E6%B3%95%E7%BB%98%E7%94%BB/">
    <div class="img-hover-zoom"><img src="/publication/%E7%BB%98%E7%94%BB%E4%BD%9C%E5%93%81%E5%AE%A1%E7%BE%8E%E8%AE%A1%E7%AE%97%E4%B8%8E%E7%AE%97%E6%B3%95%E7%BB%98%E7%94%BB/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="绘画作品审美计算与算法绘画"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E7%BB%98%E7%94%BB%E4%BD%9C%E5%93%81%E5%AE%A1%E7%BE%8E%E8%AE%A1%E7%AE%97%E4%B8%8E%E7%AE%97%E6%B3%95%E7%BB%98%E7%94%BB/">绘画作品审美计算与算法绘画</a>
  </h3>

  
  <a href="/publication/%E7%BB%98%E7%94%BB%E4%BD%9C%E5%93%81%E5%AE%A1%E7%BE%8E%E8%AE%A1%E7%AE%97%E4%B8%8E%E7%AE%97%E6%B3%95%E7%BB%98%E7%94%BB/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dongbo-liu/">Dongbo Liu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/">
    <div class="img-hover-zoom"><img src="/publication/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="神经网络小样本学习"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/">神经网络小样本学习</a>
  </h3>

  
  <a href="/publication/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/leixia/">LeiXia</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90%E7%9A%84rbm%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90%E7%9A%84rbm%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6/">
    <div class="img-hover-zoom"><img src="/publication/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90%E7%9A%84rbm%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="时间序列分析的RBM相关模型研究 "></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90%E7%9A%84rbm%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6/">时间序列分析的RBM相关模型研究 </a>
  </h3>

  
  <a href="/publication/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90%E7%9A%84rbm%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/xiaojieli/">XiaojieLi</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E7%9A%84%E5%AD%90%E7%A9%BA%E9%97%B4%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BC%82%E5%B8%B8%E7%82%B9%E6%A3%80%E6%B5%8B/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E7%9A%84%E5%AD%90%E7%A9%BA%E9%97%B4%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BC%82%E5%B8%B8%E7%82%B9%E6%A3%80%E6%B5%8B/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E7%9A%84%E5%AD%90%E7%A9%BA%E9%97%B4%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BC%82%E5%B8%B8%E7%82%B9%E6%A3%80%E6%B5%8B/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于表达的子空间学习与异常点检测"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E7%9A%84%E5%AD%90%E7%A9%BA%E9%97%B4%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BC%82%E5%B8%B8%E7%82%B9%E6%A3%80%E6%B5%8B/">基于表达的子空间学习与异常点检测</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E7%9A%84%E5%AD%90%E7%A9%BA%E9%97%B4%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BC%82%E5%B8%B8%E7%82%B9%E6%A3%80%E6%B5%8B/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/jiangshuwei/">JiangshuWei</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于表达学习的分类算法"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/">基于表达学习的分类算法</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E8%A1%A8%E8%BE%BE%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/fenghe/">FengHe</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%92%8C%E7%9B%AE%E6%A0%87%E5%BC%95%E5%AF%BC%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%AF%B9%E8%AF%9D%E6%96%B9%E6%B3%95/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%92%8C%E7%9B%AE%E6%A0%87%E5%BC%95%E5%AF%BC%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%AF%B9%E8%AF%9D%E6%96%B9%E6%B3%95/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%92%8C%E7%9B%AE%E6%A0%87%E5%BC%95%E5%AF%BC%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%AF%B9%E8%AF%9D%E6%96%B9%E6%B3%95/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于知识图谱和目标引导的主动对话方法"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%92%8C%E7%9B%AE%E6%A0%87%E5%BC%95%E5%AF%BC%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%AF%B9%E8%AF%9D%E6%96%B9%E6%B3%95/">基于知识图谱和目标引导的主动对话方法</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%92%8C%E7%9B%AE%E6%A0%87%E5%BC%95%E5%AF%BC%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%AF%B9%E8%AF%9D%E6%96%B9%E6%B3%95/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/kunxu/">KunXu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A4%B8%E5%BC%A0%E8%82%96%E5%83%8F%E6%BC%AB%E7%94%BB%E7%94%9F%E6%88%90/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A4%B8%E5%BC%A0%E8%82%96%E5%83%8F%E6%BC%AB%E7%94%BB%E7%94%9F%E6%88%90/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A4%B8%E5%BC%A0%E8%82%96%E5%83%8F%E6%BC%AB%E7%94%BB%E7%94%9F%E6%88%90/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于深度神经网络的夸张肖像漫画生成"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A4%B8%E5%BC%A0%E8%82%96%E5%83%8F%E6%BC%AB%E7%94%BB%E7%94%9F%E6%88%90/">基于深度神经网络的夸张肖像漫画生成</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A4%B8%E5%BC%A0%E8%82%96%E5%83%8F%E6%BC%AB%E7%94%BB%E7%94%9F%E6%88%90/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/lilikong/">LiliKong</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%98%E7%94%BB%E8%89%BA%E6%9C%AF%E9%A3%8E%E6%A0%BC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E5%88%86%E7%B1%BB/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%98%E7%94%BB%E8%89%BA%E6%9C%AF%E9%A3%8E%E6%A0%BC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E5%88%86%E7%B1%BB/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%98%E7%94%BB%E8%89%BA%E6%9C%AF%E9%A3%8E%E6%A0%BC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E5%88%86%E7%B1%BB/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于深度学习的绘画艺术风格特征提取与分类"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%98%E7%94%BB%E8%89%BA%E6%9C%AF%E9%A3%8E%E6%A0%BC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E5%88%86%E7%B1%BB/">基于深度学习的绘画艺术风格特征提取与分类</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%98%E7%94%BB%E8%89%BA%E6%9C%AF%E9%A3%8E%E6%A0%BC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E5%88%86%E7%B1%BB/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/huanminxu/">HuanminXu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%81%E4%B8%9A%E4%BA%91%E7%9B%98%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%81%E4%B8%9A%E4%BA%91%E7%9B%98%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%81%E4%B8%9A%E4%BA%91%E7%9B%98%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于深度学习的企业云盘中的图像分类方法"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%81%E4%B8%9A%E4%BA%91%E7%9B%98%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/">基于深度学习的企业云盘中的图像分类方法</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%81%E4%B8%9A%E4%BA%91%E7%9B%98%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/fengruidai/">FengruiDai</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E6%96%87%E4%B9%A6%E6%B3%95%E9%A3%8E%E6%A0%BC%E5%88%86%E7%B1%BB%E4%B8%8E%E7%94%9F%E6%88%90/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E6%96%87%E4%B9%A6%E6%B3%95%E9%A3%8E%E6%A0%BC%E5%88%86%E7%B1%BB%E4%B8%8E%E7%94%9F%E6%88%90/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E6%96%87%E4%B9%A6%E6%B3%95%E9%A3%8E%E6%A0%BC%E5%88%86%E7%B1%BB%E4%B8%8E%E7%94%9F%E6%88%90/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于深度学习的中文书法风格分类与生成"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E6%96%87%E4%B9%A6%E6%B3%95%E9%A3%8E%E6%A0%BC%E5%88%86%E7%B1%BB%E4%B8%8E%E7%94%9F%E6%88%90/">基于深度学习的中文书法风格分类与生成</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%AD%E6%96%87%E4%B9%A6%E6%B3%95%E9%A3%8E%E6%A0%BC%E5%88%86%E7%B1%BB%E4%B8%8E%E7%94%9F%E6%88%90/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/siwenli/">SiwenLi</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%86%B0%E7%AE%B1%E6%9E%9C%E8%94%AC%E8%AF%86%E5%88%AB/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%86%B0%E7%AE%B1%E6%9E%9C%E8%94%AC%E8%AF%86%E5%88%AB/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%86%B0%E7%AE%B1%E6%9E%9C%E8%94%AC%E8%AF%86%E5%88%AB/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于多任务卷积神经网络的冰箱果蔬识别慧择"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%86%B0%E7%AE%B1%E6%9E%9C%E8%94%AC%E8%AF%86%E5%88%AB/">基于多任务卷积神经网络的冰箱果蔬识别慧择</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%86%B0%E7%AE%B1%E6%9E%9C%E8%94%AC%E8%AF%86%E5%88%AB/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/langchen/">LangChen</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E5%88%86%E7%B1%BB%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E7%9A%84%E5%BF%83%E9%9F%B3%E8%AF%86%E5%88%AB/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E5%88%86%E7%B1%BB%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E7%9A%84%E5%BF%83%E9%9F%B3%E8%AF%86%E5%88%AB/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E5%88%86%E7%B1%BB%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E7%9A%84%E5%BF%83%E9%9F%B3%E8%AF%86%E5%88%AB/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于分类受限玻尔兹曼机的心音识别"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E5%88%86%E7%B1%BB%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E7%9A%84%E5%BF%83%E9%9F%B3%E8%AF%86%E5%88%AB/">基于分类受限玻尔兹曼机的心音识别</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E5%88%86%E7%B1%BB%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E7%9A%84%E5%BF%83%E9%9F%B3%E8%AF%86%E5%88%AB/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/zhejiang/">ZheJiang</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E4%BB%B6%E7%9B%B8%E6%9C%BA%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8E%BB%E6%A8%A1%E7%B3%8A%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E4%BB%B6%E7%9B%B8%E6%9C%BA%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8E%BB%E6%A8%A1%E7%B3%8A%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E4%BB%B6%E7%9B%B8%E6%9C%BA%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8E%BB%E6%A8%A1%E7%B3%8A%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于事件相机的图像去模糊方法研究"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E4%BB%B6%E7%9B%B8%E6%9C%BA%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8E%BB%E6%A8%A1%E7%B3%8A%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6/">基于事件相机的图像去模糊方法研究</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E4%BB%B6%E7%9B%B8%E6%9C%BA%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8E%BB%E6%A8%A1%E7%B3%8A%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/chunzhixie/">ChunzhiXie</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8Erbm%E7%9A%84%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE%E6%A8%A1%E5%9E%8B/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8Erbm%E7%9A%84%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE%E6%A8%A1%E5%9E%8B/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8Erbm%E7%9A%84%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE%E6%A8%A1%E5%9E%8B/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于RBM的特征表达模型"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8Erbm%E7%9A%84%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE%E6%A8%A1%E5%9E%8B/">基于RBM的特征表达模型</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8Erbm%E7%9A%84%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE%E6%A8%A1%E5%9E%8B/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span class="author-highlighted"><a href="/author/xiaohualu/">XiaohuaLu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%9F%BA%E4%BA%8Eitk%E5%92%8Cvtk%E7%9A%84%E6%95%B0%E5%AD%97%E5%8C%96%E5%8F%A3%E8%85%94%E9%A2%8C%E9%9D%A2%E5%A4%96%E7%A7%91%E8%BE%85%E5%8A%A9%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%9F%BA%E4%BA%8Eitk%E5%92%8Cvtk%E7%9A%84%E6%95%B0%E5%AD%97%E5%8C%96%E5%8F%A3%E8%85%94%E9%A2%8C%E9%9D%A2%E5%A4%96%E7%A7%91%E8%BE%85%E5%8A%A9%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/">
    <div class="img-hover-zoom"><img src="/publication/%E5%9F%BA%E4%BA%8Eitk%E5%92%8Cvtk%E7%9A%84%E6%95%B0%E5%AD%97%E5%8C%96%E5%8F%A3%E8%85%94%E9%A2%8C%E9%9D%A2%E5%A4%96%E7%A7%91%E8%BE%85%E5%8A%A9%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="基于ITK和VTK的数字化口腔颌面外科辅助系统设计与实现"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%9F%BA%E4%BA%8Eitk%E5%92%8Cvtk%E7%9A%84%E6%95%B0%E5%AD%97%E5%8C%96%E5%8F%A3%E8%85%94%E9%A2%8C%E9%9D%A2%E5%A4%96%E7%A7%91%E8%BE%85%E5%8A%A9%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/">基于ITK和VTK的数字化口腔颌面外科辅助系统设计与实现</a>
  </h3>

  
  <a href="/publication/%E5%9F%BA%E4%BA%8Eitk%E5%92%8Cvtk%E7%9A%84%E6%95%B0%E5%AD%97%E5%8C%96%E5%8F%A3%E8%85%94%E9%A2%8C%E9%9D%A2%E5%A4%96%E7%A7%91%E8%BE%85%E5%8A%A9%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-7 year-2015">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/dayiheng-liu/">Dayiheng Liu</a></span>, <span ><a href="/author/jiancheng-lv/">Jiancheng Lv</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>STC</em>
    
  </span>
  

  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="/publication/%E5%8F%AF%E6%8E%A7%E5%BC%8F%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%B8%8E%E6%94%B9%E5%86%99%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/#disqus_thread"></a>
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/%E5%8F%AF%E6%8E%A7%E5%BC%8F%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%B8%8E%E6%94%B9%E5%86%99%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/">
    <div class="img-hover-zoom"><img src="/publication/%E5%8F%AF%E6%8E%A7%E5%BC%8F%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%B8%8E%E6%94%B9%E5%86%99%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/featured_hu0c725bf57ece83f021949ee94fa293e5_46863_918x517_fill_q90_lanczos_smart1.JPG" class="article-banner" alt="可控式文本生成与改写的神经网络方法"></div>
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/%E5%8F%AF%E6%8E%A7%E5%BC%8F%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%B8%8E%E6%94%B9%E5%86%99%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/">可控式文本生成与改写的神经网络方法</a>
  </h3>

  
  <a href="/publication/%E5%8F%AF%E6%8E%A7%E5%BC%8F%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%B8%8E%E6%94%B9%E5%86%99%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%96%B9%E6%B3%95/" class="summary-link">
    <div class="article-style">
      <p>介绍</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  









  
    
  




  






  </div>
  

</div>

          
        </div>

        
      </div>

    </div>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    
    
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    

    
    

    
    
    <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/wowchemy.min.bbce1786921b11de61ce2cf40ecf1421.js"></script>

    






</body>
</html>
